1. 领域文档选择与收集（第一步，确定数据源）
这是所有工作的基础，先明确 “用什么数据”。
第一步：和团队对齐领域。比如选 “计算机网络”“金融合规”“医疗常识” 等垂直领域，确保和 RAG 系统的应用场景匹配（避免数据太泛）。
第二步：确定文档来源。优先选免费、无版权风险的渠道，比如：
官方公开文档：政府 / 企业官网的白皮书、技术文档（如 Python 官方文档、AWS 技术手册）。
学术资源：arXiv（论文）、IEEE Xplore（需免费开放版）、大学公开课讲义。
开源平台：GitHub 仓库的 README、Wiki 文档；GitBook 上的公开书籍。
第三步：批量收集。目标是至少收集 50-100 份文档（保证数据量），可用工具：
少量文档：直接手动下载（PDF 格式为主）。
大量文档：用 Python 爬虫（如 Requests、Scrapy）爬取公开页面，但要遵守网站的 robots 协议（避免侵权）。


2. 数据清洗与格式化（核心步骤，把 “raw 数据” 变 “可用数据”）
目标是把收集的 PDF 文档，转换成 RAG 系统能处理的 “JSON Lines（jsonl）” 格式，中间需经过 “PDF→Markdown（md）→JSONL” 两步转换。
第一步：PDF 转 Markdown（解决 PDF 格式混乱问题）。
工具：优先用 Python 库，比如pdfplumber（精准提取文字，保留格式）、PyPDF2（简单提取，适合纯文字 PDF）；如果手动处理，可用在线工具（如 SmallPDF、Convertio）。
关键：提取后检查是否有 “乱码、换行错误、表格丢失”，比如把 PDF 里的表格转换成 Markdown 表格格式。
第二步：Markdown 清洗（去噪声）。
删无关内容：比如文档页眉页脚、广告、重复的封面 / 目录。
统一格式：把标题层级对齐（如# 一级标题、## 二级标题），删除多余空行，确保段落连贯。
第三步：Markdown 转 JSONL（RAG 系统标准输入格式）。
格式要求：每个文档（或每个分块）对应 JSONL 里的一行，包含核心字段，示例：
json
{"doc_id": "net-001", "content": "TCP协议是一种面向连接的、可靠的传输层协议...", "metadata": {"title": "计算机网络基础", "source": "大学教材", "page": 25}}
工具：用 Python 脚本（如json库）批量处理，遍历所有 md 文件，读取内容后按上述格式写入.jsonl文件。


3. 研究分块策略并测试（关键优化点，影响后续检索效果）
分块是把长文档拆成短文本（比如 500 字 / 块），不同策略会直接影响 RAG 的 “召回率”，需要对比测试。
第一步：调研 3 种核心分块策略。
长度分块（基础）：按固定字符数 /token 数拆分（如每 512 token 一块），适合结构简单的文档，工具用 LangChain 的RecursiveCharacterTextSplitter。
语义分块（进阶）：按句子语义关联性拆分（比如 “一个段落讲完一个主题就拆分”），避免把完整语义拆断，工具用 LangChain 的SentenceTransformersTokenTextSplitter或ChatGPTTokenizerTextSplitter。
父子分块（复杂）：先把文档拆成 “父块”（如一个章节），再把父块拆成 “子块”（如章节里的段落），保留层级关系，适合有明确结构的文档（如教材、技术手册）。
第二步：代码实现与对比。
用 Python（LangChain 或 LlamaIndex 框架）实现 3 种分块策略，对同一批文档进行拆分。
记录关键指标：每个策略的分块数量、平均块长度、是否出现 “语义断裂”（比如把一个句子拆到两个块里）。


4. 准备测评数据集（对接评测组，为后续评估做准备）
测评数据集是用来测试 RAG 系统 “好不好用” 的依据，需和负责 “实验评估” 的同学对齐格式。
两种形式二选一（按评测组需求）：
形式 1：QA 对（问题 + 标准答案）。比如问题 “TCP 和 UDP 的区别是什么？”，标准答案 “1. 连接性：TCP 面向连接，UDP 无连接...”。
形式 2：仅问题（需标注答案来源）。比如问题 “什么是 HTTP 状态码 404？”，标注答案来自 “文档 net-003 的第 18 页”。
构建方法：
手动构建：从分块后的文本里，针对核心知识点设计 100-200 个问题（确保覆盖领域重点）。
工具辅助：用开源模型（如 Qwen-7B、Llama 3）批量生成问题，再人工筛选修正（提高效率）。


二、你需要学习的知识与工具（按 “急用先学” 排序）
1. 必学工具（直接用得上，1-2 天能上手）
Python 基础操作：熟练用os库遍历文件、json库处理 JSONL 格式、pandas库查看数据（检查清洗后的数据质量）。
文档处理库：
pdfplumber：重点学 “提取页面文字”“提取表格” 的代码（官方文档有示例，直接抄改）。
LangChain：重点学 “文本分块器（TextSplitter）” 的使用（官网有现成的分块代码，换参数就能测不同策略）。
数据格式常识：了解 JSONL 和 JSON 的区别（JSONL 是每行一个 JSON 对象，适合大文件批量处理）、Markdown 的基础语法（标题、表格、列表）。
2. 辅助知识（理解 “为什么这么做”，提升质量）
RAG 基础逻辑：知道 “数据预处理→编码→检索→生成” 的流程，明白 “分块策略影响检索召回率” 的原因（比如分块太碎会丢上下文，太大会包含冗余信息）。
数据质量标准：知道 “干净的语料” 要满足什么条件 —— 无乱码、无重复、语义完整、元数据准确（比如来源、页码）。
版权常识：避免用有版权的文档（如付费电子书、未公开的企业文档），优先选 CC 协议（知识共享协议）下的公开资源。

三、你的最终输出物（要交付给团队的成果）
干净的原始语料：整理好的 PDF 文档文件夹（按领域分类，命名规范，如 “计算机网络 - 原始 PDF/”），附带一份 “语料说明文档”（写清楚来源、数量、领域范围）。
分块后的文本：3 个版本的 JSONL 文件（对应 3 种分块策略，命名如 “chunk_length.jsonl”“chunk_semantic.jsonl”），附带一份 “分块报告”（说明每种策略的参数、分块数量、优缺点）。
测评数据集：1 个 JSONL 或 Excel 文件（包含问题、标准答案 / 答案来源），格式和评测组对齐，确保每个问题都能在分块文本中找到答案。