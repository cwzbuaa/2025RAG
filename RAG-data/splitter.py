from langchain_text_splitters import (
    RecursiveCharacterTextSplitter,  # é•¿åº¦åˆ†å—
    SentenceTransformersTokenTextSplitter  # è¯­ä¹‰åˆ†å—
)
import json
import os
from datetime import datetime
from collections import defaultdict

# 1. åŠ è½½åŸå§‹JSONLè¯­æ–™
def load_corpus(jsonl_path):
    """åŠ è½½å•ä¸ªJSONLæ–‡ä»¶"""
    corpus = []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            corpus.append(json.loads(line))
    return corpus

# 2. æ‰¹é‡åŠ è½½æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰JSONLæ–‡ä»¶
def load_all_corpus(corpus_dir):
    """
    æ‰¹é‡åŠ è½½æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰JSONLæ–‡ä»¶
    :param corpus_dir: JSONLæ–‡ä»¶æ‰€åœ¨ç›®å½•
    :return: æ‰€æœ‰æ–‡æ¡£çš„åˆ—è¡¨
    """
    all_corpus = []
    jsonl_files = [f for f in os.listdir(corpus_dir) if f.endswith('.jsonl')]
    
    print(f"ğŸ“‚ å‘ç° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶:")
    for jsonl_file in jsonl_files:
        file_path = os.path.join(corpus_dir, jsonl_file)
        corpus = load_corpus(file_path)
        all_corpus.extend(corpus)
        print(f"  âœ“ {jsonl_file}: {len(corpus)} ä¸ªæ–‡æ¡£")
    
    print(f"\nğŸ“Š æ€»è®¡åŠ è½½: {len(all_corpus)} ä¸ªæ–‡æ¡£\n")
    return all_corpus

# 3. åˆ†å—ç»Ÿè®¡ç±»
class ChunkStatistics:
    """åˆ†å—ç»Ÿè®¡åˆ†æå™¨"""
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.total_chunks = 0
        self.chunk_lengths = []
        self.doc_chunk_counts = defaultdict(int)
        self.semantic_breaks = 0  # è¯­ä¹‰æ–­è£‚æ¬¡æ•°ï¼ˆç®€å•æ£€æµ‹ï¼šå—ç»“å°¾ä¸æ˜¯å¥å·/é—®å·/æ„Ÿå¹å·ï¼‰
    
    def add_chunk(self, doc_id, chunk_content):
        """æ·»åŠ ä¸€ä¸ªåˆ†å—çš„ç»Ÿè®¡ä¿¡æ¯"""
        self.total_chunks += 1
        self.chunk_lengths.append(len(chunk_content))
        self.doc_chunk_counts[doc_id] += 1
        
        # æ£€æµ‹è¯­ä¹‰æ–­è£‚ï¼ˆç®€å•è§„åˆ™ï¼šå—ç»“å°¾ä¸æ˜¯å®Œæ•´å¥å­ï¼‰
        if chunk_content.strip() and chunk_content.strip()[-1] not in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', '\n']:
            self.semantic_breaks += 1
    
    def get_report(self, strategy_name):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        if not self.chunk_lengths:
            return "æ— åˆ†å—æ•°æ®"
        
        avg_length = sum(self.chunk_lengths) / len(self.chunk_lengths)
        min_length = min(self.chunk_lengths)
        max_length = max(self.chunk_lengths)
        
        report = f"""
{'='*60}
åˆ†å—ç­–ç•¥: {strategy_name}
{'='*60}
ğŸ“Š åŸºæœ¬ç»Ÿè®¡:
  - æ€»åˆ†å—æ•°: {self.total_chunks}
  - å¹³å‡å—é•¿åº¦: {avg_length:.0f} å­—ç¬¦
  - æœ€çŸ­å—: {min_length} å­—ç¬¦
  - æœ€é•¿å—: {max_length} å­—ç¬¦
  - åŸå§‹æ–‡æ¡£æ•°: {len(self.doc_chunk_counts)}
  - å¹³å‡æ¯æ–‡æ¡£åˆ†å—æ•°: {self.total_chunks / len(self.doc_chunk_counts):.1f}

âš ï¸  è´¨é‡æŒ‡æ ‡:
  - ç–‘ä¼¼è¯­ä¹‰æ–­è£‚: {self.semantic_breaks} ä¸ª ({self.semantic_breaks/self.total_chunks*100:.1f}%)
  
ğŸ“ˆ é•¿åº¦åˆ†å¸ƒ:
  - 0-256å­—ç¬¦: {sum(1 for l in self.chunk_lengths if l <= 256)} ä¸ª
  - 257-512å­—ç¬¦: {sum(1 for l in self.chunk_lengths if 256 < l <= 512)} ä¸ª
  - 513-768å­—ç¬¦: {sum(1 for l in self.chunk_lengths if 512 < l <= 768)} ä¸ª
  - 769+å­—ç¬¦: {sum(1 for l in self.chunk_lengths if l > 768)} ä¸ª
{'='*60}
"""
        return report

# 4. å®ç°3ç§åˆ†å—ç­–ç•¥ï¼ˆå¸¦ç»Ÿè®¡ï¼‰
def chunk_corpus(corpus, chunk_strategy, save_chunked_jsonl):
    """
    å¯¹è¯­æ–™è¿›è¡Œåˆ†å—
    :param corpus: åŠ è½½åçš„åŸå§‹è¯­æ–™ï¼ˆlist of dictï¼‰
    :param chunk_strategy: åˆ†å—ç­–ç•¥ï¼ˆ"length"/"semantic"/"parent_child"ï¼‰
    :param save_chunked_jsonl: åˆ†å—åJSONLä¿å­˜è·¯å¾„
    :return: ç»Ÿè®¡å¯¹è±¡
    """
    stats = ChunkStatistics()
    
    # åˆå§‹åŒ–åˆ†å—å™¨
    if chunk_strategy == "length":
        # é•¿åº¦åˆ†å—ï¼šæ¯512ä¸ªå­—ç¬¦ä¸€å—ï¼Œé‡å 50å­—ç¬¦ï¼ˆé¿å…æ–­å¥ï¼‰
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=512,
            chunk_overlap=50,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
    elif chunk_strategy == "semantic":
        # è¯­ä¹‰åˆ†å—ï¼šç”¨å¤šè¯­è¨€Sentence-BERTæ¨¡å‹ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        text_splitter = SentenceTransformersTokenTextSplitter(
            chunk_size=512,
            chunk_overlap=50,
            model_name="paraphrase-multilingual-MiniLM-L12-v2"  # å¤šè¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒä¸­æ–‡
        )
    elif chunk_strategy == "parent_child":
        # çˆ¶å­åˆ†å—ï¼šå…ˆæŒ‰1024å­—ç¬¦åˆ†çˆ¶å—ï¼Œå†æŒ‰256å­—ç¬¦åˆ†å­å—
        parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1024, 
            chunk_overlap=100,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=256, 
            chunk_overlap=20,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
    
    # æ‰§è¡Œåˆ†å—å¹¶ä¿å­˜
    print(f"ğŸ”„ å¼€å§‹ {chunk_strategy} åˆ†å—...")
    with open(save_chunked_jsonl, "w", encoding="utf-8") as f:
        for doc in corpus:
            if chunk_strategy != "parent_child":
                # é•¿åº¦/è¯­ä¹‰åˆ†å—ï¼šç›´æ¥åˆ†
                chunks = text_splitter.split_text(doc["content"])
                for i, chunk in enumerate(chunks):
                    chunk_obj = {
                        "doc_id": doc["doc_id"],
                        "chunk_id": f"{doc['doc_id']}_chunk{i+1}",
                        "content": chunk,
                        "metadata": doc["metadata"],
                        "chunk_strategy": chunk_strategy
                    }
                    f.write(json.dumps(chunk_obj, ensure_ascii=False) + "\n")
                    stats.add_chunk(doc["doc_id"], chunk)
            else:
                # çˆ¶å­åˆ†å—ï¼šå…ˆåˆ†çˆ¶å—ï¼Œå†åˆ†å­å—
                parent_chunks = parent_splitter.split_text(doc["content"])
                for p_idx, parent_chunk in enumerate(parent_chunks):
                    child_chunks = child_splitter.split_text(parent_chunk)
                    for c_idx, child_chunk in enumerate(child_chunks):
                        chunk_obj = {
                            "doc_id": doc["doc_id"],
                            "parent_chunk_id": f"{doc['doc_id']}_parent{p_idx+1}",
                            "child_chunk_id": f"{doc['doc_id']}_parent{p_idx+1}_child{c_idx+1}",
                            "content": child_chunk,
                            "parent_content": parent_chunk,  # ä¿ç•™çˆ¶å—å†…å®¹ï¼Œç”¨äºå±‚çº§å…³è”
                            "metadata": doc["metadata"],
                            "chunk_strategy": chunk_strategy
                        }
                        f.write(json.dumps(chunk_obj, ensure_ascii=False) + "\n")
                        stats.add_chunk(doc["doc_id"], child_chunk)
    
    print(f"âœ… {chunk_strategy} åˆ†å—å®Œæˆï¼Œä¿å­˜è·¯å¾„ï¼š{save_chunked_jsonl}")
    return stats

# 5. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
def generate_comparison_report(all_stats, output_path):
    """
    ç”Ÿæˆ3ç§ç­–ç•¥çš„å¯¹æ¯”æŠ¥å‘Š
    :param all_stats: å­—å…¸ï¼Œkeyä¸ºç­–ç•¥åï¼Œvalueä¸ºç»Ÿè®¡å¯¹è±¡
    :param output_path: æŠ¥å‘Šä¿å­˜è·¯å¾„
    """
    report_lines = [
        "=" * 80,
        "åˆ†å—ç­–ç•¥å¯¹æ¯”æŠ¥å‘Š",
        f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "=" * 80,
        ""
    ]
    
    # æ·»åŠ æ¯ä¸ªç­–ç•¥çš„è¯¦ç»†æŠ¥å‘Š
    for strategy, stats in all_stats.items():
        report_lines.append(stats.get_report(strategy))
    
    # æ·»åŠ å¯¹æ¯”æ€»ç»“
    report_lines.append("\n" + "=" * 80)
    report_lines.append("ğŸ“‹ ç­–ç•¥å¯¹æ¯”æ€»ç»“")
    report_lines.append("=" * 80)
    
    comparison_table = "\n{:<20} {:<15} {:<15} {:<15}".format(
        "ç­–ç•¥", "æ€»åˆ†å—æ•°", "å¹³å‡å—é•¿åº¦", "è¯­ä¹‰æ–­è£‚ç‡"
    )
    comparison_table += "\n" + "-" * 80
    
    for strategy, stats in all_stats.items():
        if stats.chunk_lengths:
            avg_len = sum(stats.chunk_lengths) / len(stats.chunk_lengths)
            break_rate = stats.semantic_breaks / stats.total_chunks * 100
            comparison_table += "\n{:<20} {:<15} {:<15.0f} {:<15.1f}%".format(
                strategy, stats.total_chunks, avg_len, break_rate
            )
    
    report_lines.append(comparison_table)
    
    # æ·»åŠ å»ºè®®
    report_lines.append("\n\nğŸ’¡ ç­–ç•¥é€‰æ‹©å»ºè®®:")
    report_lines.append("-" * 80)
    report_lines.append("â€¢ length (é•¿åº¦åˆ†å—): é€‚åˆç»“æ„ç®€å•çš„æ–‡æ¡£ï¼Œé€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½æ–­å¥")
    report_lines.append("â€¢ semantic (è¯­ä¹‰åˆ†å—): é€‚åˆä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Œä½†è®¡ç®—æˆæœ¬é«˜")
    report_lines.append("â€¢ parent_child (çˆ¶å­åˆ†å—): é€‚åˆæœ‰å±‚çº§ç»“æ„çš„æ–‡æ¡£ï¼Œä¿ç•™ä¸Šä¸‹æ–‡å…³ç³»")
    report_lines.append("=" * 80)
    
    # ä¿å­˜æŠ¥å‘Š
    report_content = "\n".join(report_lines)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(report_content)
    
    print(f"\nğŸ“„ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    print(report_content)

# ============ ä¸»ç¨‹åº ============
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    CORPUS_DIR = "jsonl_corpus/howtocook"  # JSONLæ–‡ä»¶æ‰€åœ¨ç›®å½•
    OUTPUT_DIR = "chunked_corpus/howtocook"  # åˆ†å—ç»“æœè¾“å‡ºç›®å½•
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("=" * 80)
    print("ğŸš€ å¼€å§‹åˆ†å—å¤„ç†")
    print("=" * 80)
    
    # 1. æ‰¹é‡åŠ è½½æ‰€æœ‰JSONLæ–‡ä»¶
    corpus = load_all_corpus(CORPUS_DIR)
    
    if not corpus:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼")
        exit(1)
    
    # 2. æ‰¹é‡æ‰§è¡Œ3ç§åˆ†å—ç­–ç•¥
    strategies = ["length", "semantic", "parent_child"]
    all_stats = {}
    
    for strategy in strategies:
        save_path = os.path.join(OUTPUT_DIR, f"chunked_corpus_{strategy}.jsonl")
        stats = chunk_corpus(corpus, strategy, save_path)
        all_stats[strategy] = stats
        print()  # ç©ºè¡Œåˆ†éš”
    
    # 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    report_path = os.path.join(OUTPUT_DIR, "chunking_report.txt")
    generate_comparison_report(all_stats, report_path)
    
    print("\nâœ¨ æ‰€æœ‰åˆ†å—ä»»åŠ¡å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ“„ å¯¹æ¯”æŠ¥å‘Š: {report_path}")