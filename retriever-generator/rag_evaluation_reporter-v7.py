import os
import json
import time
import pandas as pd
import traceback  # ç”¨äºè¯¦ç»†é”™è¯¯è·Ÿè¸ª
import nest_asyncio
nest_asyncio.apply()

# --- RAGASè¯„ä¼°ç›¸å…³ç»„ä»¶ ---
from ragas import evaluate
from ragas.metrics import (
    context_recall,
    context_precision,
    faithfulness,
    answer_relevancy
)
from datasets import Dataset  # ç”¨äºè§£å†³ DataFrame è½¬æ¢é—®é¢˜

# --- RAGAS LLMé…ç½® ---
from ragas.llms import LangchainLLMWrapper
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import DashScopeEmbeddings

# --- ç›´æ¥å¯¼å…¥é¢„æ„å»ºçš„RAGç³»ç»Ÿ ---
from agent_backend import load_agent_executor

# =============== é…ç½®éƒ¨åˆ† ===============
# ç»Ÿä¸€API_KEYï¼ˆä¸agent_backend.pyä¿æŒä¸€è‡´ï¼‰
API_KEY = ""  # å¿…é¡»ä¸agent_backend.pyä¸­çš„API_KEYä¸€è‡´
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"

# è¯„ä¼°é…ç½®
TEST_DATA_PATH = "ceshi.jsonl"
REPORT_DIR = "evaluation_results"
os.makedirs(REPORT_DIR, exist_ok=True)

# é€šç”¨AIé…ç½®ï¼ˆä¸RAGç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ä»¥ä¿è¯å…¬å¹³æ¯”è¾ƒï¼‰
GENERAL_MODEL = "qwen-flash"
GENERAL_TEMPERATURE = 0.1

# RAGASè¯„ä¼°ä¸“ç”¨é…ç½®
RAGAS_EVALUATION_MODEL = "qwen-turbo"  # ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°
RAGAS_TEMPERATURE = 0.1


# =============== é…ç½®RAGASä½¿ç”¨çš„LLM ===============
def create_ragas_llm():

    # åˆ›å»ºLangChainå…¼å®¹çš„LLMå®ä¾‹ï¼ˆé€šä¹‰åƒé—®ï¼‰
    llm = ChatOpenAI(
        model_name=RAGAS_EVALUATION_MODEL,
        temperature=RAGAS_TEMPERATURE,
        api_key=API_KEY,
        base_url=BASE_URL
    )

    # åŒ…è£…ä¸ºRAGASå…¼å®¹çš„LLM
    return LangchainLLMWrapper(llm)


# =============== æ•°æ®åŠ è½½å‡½æ•° ===============

def load_test_questions(file_path: str) -> list:
    """åŠ è½½æµ‹è¯•é—®é¢˜é›†JSONLæ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰"""
    questions = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():  # è·³è¿‡ç©ºè¡Œ
                    data = json.loads(line.strip())
                    questions.append(data)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(questions)} ä¸ªæµ‹è¯•é—®é¢˜")
        return questions
    except Exception as e:
        print(f"âŒ åŠ è½½æµ‹è¯•é—®é¢˜å¤±è´¥: {str(e)}")
        traceback.print_exc()
        raise


# =============== RAGç³»ç»Ÿè¯„ä¼°å‡½æ•° ===============

def run_rag_system(test_questions: list):

    print("\nğŸ”„ æ­£åœ¨åŠ è½½é¢„æ„å»ºçš„RAGç³»ç»Ÿï¼ˆæ¥è‡ªagent_backend.pyï¼‰...")

    try:
        # ç›´æ¥è°ƒç”¨agent_backend.pyä¸­çš„å‡½æ•°ï¼Œåˆ›å»ºå®Œæ•´çš„RAGç³»ç»Ÿ
        # è¿™ç¡®ä¿äº†è¯„ä¼°ä½¿ç”¨çš„æ˜¯ä¸å®é™…éƒ¨ç½²å®Œå…¨ç›¸åŒçš„ç³»ç»Ÿ
        # ====== ä¿®å¤å…³é”®ç‚¹ï¼šè®¾ç½®DashScope API Keyç¯å¢ƒå˜é‡ ======
        os.environ["DASHSCOPE_API_KEY"] = API_KEY  # ä½¿ç”¨ç›¸åŒçš„API_KEY
        print(f"âœ… å·²è®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡ (å‰10ä½: {API_KEY[:10]}...)")  # é¿å…æ‰“å°å®Œæ•´å¯†é’¥
        # ====================================================

        from agent_backend import load_agent_executor  # ç°åœ¨ç¯å¢ƒå˜é‡å·²è®¾ç½®
        print("âœ… æ­£åœ¨åŠ è½½RAGç³»ç»Ÿ...")
        agent_executor, retriever, memory = load_agent_executor(
            enable_rerank=True,  # å¯ç”¨é‡æ’åŠŸèƒ½
            rerank_model="qwen3-rerank",
            top_k=5,  # å…ˆå¬å›5ä¸ªæ–‡æ¡£
            rerank_top_n=3  # é‡æ’åè¿”å›3ä¸ªæœ€ç›¸å…³æ–‡æ¡£
        )
        print("âœ… RAGç³»ç»ŸåŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ åŠ è½½RAGç³»ç»Ÿå¤±è´¥: {str(e)}")
        traceback.print_exc()
        raise

    results = []
    total_time = 0
    failed_questions = 0

    print(f"\nğŸ” å¼€å§‹RAGç³»ç»Ÿæµ‹è¯•ï¼ˆå…± {len(test_questions)} ä¸ªé—®é¢˜ï¼‰...")

    for i, q in enumerate(test_questions, 1):
        question_text = q["Question_Text"]
        print(f"  [é—®é¢˜ {i}/{len(test_questions)}] {question_text[:50]}{'...' if len(question_text) > 50 else ''}")

        try:
            start_time = time.time()

            # ä½¿ç”¨é¢„æ„å»ºçš„Agentæ‰§è¡Œå™¨å¤„ç†é—®é¢˜
            # æ³¨æ„ï¼šAgentExecutorçš„è¾“å…¥æ˜¯{"input": "é—®é¢˜æ–‡æœ¬"}
            result = agent_executor.invoke({"input": question_text})

            # ä»Agentç»“æœä¸­æå–æœ€ç»ˆç­”æ¡ˆ
            answer = result.get("output", "")

            # ä» intermediate_steps æå–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆå‚è€ƒ app.py çš„å¤„ç†æ–¹å¼ï¼‰
            context_list = []
            if "intermediate_steps" in result and result["intermediate_steps"]:
                for step in result["intermediate_steps"]:
                    if isinstance(step, tuple) and len(step) == 2:
                        action, observation = step
                        # LangChain AgentAction å¸¸ç”¨å±æ€§ä¸º .toolï¼Œè€Œé .name
                        if hasattr(action, "tool") and action.tool == "search_recipe_database":
                            if isinstance(observation, list):
                                for doc in observation:
                                    if hasattr(doc, "page_content"):
                                        context_list.append(doc.page_content)
                            elif hasattr(observation, "page_content"):
                                context_list.append(observation.page_content)
                            elif isinstance(observation, str):
                                context_list.append(observation)

            # å°†ä¸Šä¸‹æ–‡åˆ—è¡¨åˆå¹¶ä¸ºå­—ç¬¦ä¸²
            # è‹¥æœªæ•è·åˆ°ä¸Šä¸‹æ–‡ï¼Œå›é€€ç›´æ¥è°ƒç”¨ retriever
            if not context_list:
                try:
                    fallback_docs = retriever.invoke(question_text)
                    if isinstance(fallback_docs, list):
                        context_list = [
                            getattr(d, "page_content", str(d)) for d in fallback_docs
                        ]
                    elif hasattr(fallback_docs, "page_content"):
                        context_list = [fallback_docs.page_content]
                    elif isinstance(fallback_docs, str):
                        context_list = [fallback_docs]
                except Exception:
                    pass
            context_str = "\n".join(context_list) if context_list else ""

            # è®¡ç®—å¤„ç†æ—¶é—´
            elapsed = time.time() - start_time
            total_time += elapsed

            # ä¿å­˜ç»“æœ
            results.append({
                "question": question_text,
                "answer": answer,
                "context_list": context_list,
                "context_str": context_str,
                "standard_answer": q["Ground_Truth"],
                "time": elapsed,
                "success": True
            })

        except Exception as e:
            print(f"    âŒ å¤„ç†é—®é¢˜å¤±è´¥: {str(e)}")
            traceback.print_exc()
            failed_questions += 1

            # ä¿å­˜å¤±è´¥ç»“æœ
            results.append({
                "question": question_text,
                "answer": f"å¤„ç†å¤±è´¥: {str(e)}",
                "context_list": [],
                "context_str": "",
                "standard_answer": q["Ground_Truth"],
                "time": 0,
                "success": False
            })

    if failed_questions > 0:
        print(f"\nâš ï¸  è­¦å‘Š: {failed_questions} ä¸ªé—®é¢˜å¤„ç†å¤±è´¥ï¼Œå·²è®°å½•é”™è¯¯ä¿¡æ¯")

    avg_time = total_time / (len(test_questions) - failed_questions) if (
                                                                                    len(test_questions) - failed_questions) > 0 else 0
    print(f"âœ… RAGç³»ç»Ÿæµ‹è¯•å®Œæˆ! å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")

    return results, total_time


# =============== é€šç”¨AIè¯„ä¼°å‡½æ•° ===============

def create_general_llm():
    """åˆ›å»ºé€šç”¨AIæ¨¡å‹ï¼ˆä¸RAGç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„æ¨¡å‹é…ç½®ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼‰"""
    # ä»langchain_openaiå¯¼å…¥ChatOpenAI
    from langchain_openai import ChatOpenAI

    # ä½¿ç”¨ä¸RAGç³»ç»Ÿç›¸åŒçš„æ¨¡å‹å’Œå‚æ•°
    return ChatOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        model=GENERAL_MODEL,  # ä¸RAGç³»ç»Ÿç›¸åŒçš„æ¨¡å‹
        temperature=GENERAL_TEMPERATURE
    )


def run_general_ai(test_questions: list):
    """è¿è¡Œé€šç”¨AIæµ‹è¯•ï¼ˆæ— æ£€ç´¢åŠŸèƒ½ï¼‰"""
    print("\nğŸ”„ åˆå§‹åŒ–é€šç”¨AIæ¨¡å‹...")
    try:
        general_llm = create_general_llm()
        print("âœ… é€šç”¨AIæ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ åŠ è½½é€šç”¨AIæ¨¡å‹å¤±è´¥: {str(e)}")
        traceback.print_exc()
        raise

    results = []
    total_time = 0
    failed_questions = 0
    print(f"\nğŸ” å¼€å§‹é€šç”¨AIæµ‹è¯•ï¼ˆå…± {len(test_questions)} ä¸ªé—®é¢˜ï¼‰...")

    for i, q in enumerate(test_questions, 1):
        question_text = q["Question_Text"]
        print(f" [é—®é¢˜ {i}/{len(test_questions)}] {question_text[:50]}{'...' if len(question_text) > 50 else ''}")

        try:
            start_time = time.time()
            # âœ… ä¿®æ”¹æç¤ºè¯ï¼Œæ˜ç¡®æŒ‡ä»¤
            prompt = (
                "ä½ æ˜¯ä¸€ä¸ªAIçƒ¹é¥ªåŠ©æ‰‹ï¼Œåå«â€œèˆªå°å¨â€ã€‚ä½ çš„ä»»åŠ¡æ˜¯å›ç­”å…³äºèœè°±ã€é£Ÿæå’Œçƒ¹é¥ªæŠ€å·§çš„é—®é¢˜ã€‚\n"
                "é‡è¦è§„åˆ™ï¼š\n"
                "1. å¦‚æœé—®é¢˜æ˜¯å…³äºçƒ¹é¥ªçš„ï¼Œè¯·åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å›ç­”ã€‚\n"
                "2. å¦‚æœé—®é¢˜ä¸çƒ¹é¥ªæ— å…³ï¼ˆå¦‚æ–‡å­¦ã€å†å²ã€ç§‘æŠ€ç­‰ï¼‰ï¼Œä½ å¿…é¡»å›ç­”ï¼š"
                "\"æˆ‘æ˜¯ä¸€ä¸ªAIçƒ¹é¥ªåŠ©æ‰‹èˆªå°å¨ï¼Œæˆ‘çš„ä»»åŠ¡æ˜¯å¸®æ‚¨å¤„ç†å…³äºèœè°±çš„é—®é¢˜ã€‚å¾ˆæŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”çƒ¹é¥ªä¹‹å¤–çš„é—®é¢˜ã€‚\"\n\n"
                f"é—®é¢˜ï¼š{question_text}\n\n"
                "å›ç­”ï¼š"
            )

            response = general_llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)

            elapsed = time.time() - start_time
            total_time += elapsed

            results.append({
                "question": question_text,
                "answer": answer,
                "context_list": [],
                "context_str": "",
                "standard_answer": q["Ground_Truth"],
                "time": elapsed,
                "success": True
            })

        except Exception as e:
            print(f" âŒ å¤„ç†é—®é¢˜å¤±è´¥: {str(e)}")
            traceback.print_exc()
            failed_questions += 1
            results.append({
                "question": question_text,
                "answer": f"å¤„ç†å¤±è´¥: {str(e)}",
                "context_list": [],
                "context_str": "",
                "standard_answer": q["Ground_Truth"],
                "time": 0,
                "success": False
            })

    if failed_questions > 0:
        print(f"\nâš ï¸ è­¦å‘Š: {failed_questions} ä¸ªé—®é¢˜å¤„ç†å¤±è´¥ï¼Œå·²è®°å½•é”™è¯¯ä¿¡æ¯")
    avg_time = total_time / (len(test_questions) - failed_questions) if (len(test_questions) - failed_questions) > 0 else 0
    print(f"âœ… é€šç”¨AIæµ‹è¯•å®Œæˆ! å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")
    return results, total_time


# =============== RAGASæŒ‡æ ‡è®¡ç®—å‡½æ•° ===============
def calculate_ragas_metrics(results: list, is_rag: bool = True):
    """
    è®¡ç®—RAGASè¯„ä¼°æŒ‡æ ‡
    å‚æ•°:
        results: è¯„ä¼°ç»“æœåˆ—è¡¨
        is_rag: æ˜¯å¦ä¸ºRAGç³»ç»Ÿï¼ˆå½±å“è®¡ç®—å“ªäº›æŒ‡æ ‡ï¼‰
    """
    print(f"\nğŸ“Š è®¡ç®—{'RAG' if is_rag else 'é€šç”¨AI'}ç³»ç»ŸRAGASæŒ‡æ ‡...")

    # 1. åˆ›å»ºRAGASä¸“ç”¨çš„LLMï¼ˆå…³é”®ä¿®å¤ï¼šé¿å…ä½¿ç”¨OpenAIï¼‰
    try:
        ragas_llm = create_ragas_llm()
        print(f"âœ… RAGASè¯„ä¼°LLMé…ç½®æˆåŠŸ! æ¨¡å‹: {RAGAS_EVALUATION_MODEL}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºRAGASè¯„ä¼°LLMå¤±è´¥: {str(e)}")
        traceback.print_exc()
        # è¿”å›ç©ºæŒ‡æ ‡
        empty_metrics = {
            "context_recall": None,
            "context_precision": None,
            "faithfulness": None,
            "answer_relevancy": None
        }
        return results, empty_metrics

        # 2. å‡†å¤‡æ•°æ®ï¼ˆè¿‡æ»¤æ‰å¤±è´¥çš„é—®é¢˜ï¼‰
    valid_results = [r for r in results if r.get("success", True)]
    if len(valid_results) == 0:
        print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„ç»“æœå¯ç”¨äºè®¡ç®—æŒ‡æ ‡")
        empty_metrics = {
            "context_recall": None,
            "context_precision": None,
            "faithfulness": None,
            "answer_relevancy": None
        }
        return results, empty_metrics

    data = []
    for r in valid_results:
        # å‡†å¤‡ä¸Šä¸‹æ–‡ï¼šRAGAS è¦æ±‚ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¼˜å…ˆä½¿ç”¨ context_list
        contexts = r.get("context_list") if is_rag else []
        if not isinstance(contexts, list):
            contexts = [str(contexts)] if contexts else []
        # ç¡®ä¿ contexts æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
        contexts = [str(c) for c in contexts] if contexts else []
        # å¦‚æœä¸ºç©ºï¼Œç”¨å ä½
        contexts = contexts if contexts else ["æ— ä¸Šä¸‹æ–‡"]

        # å‡†å¤‡æ ‡å‡†ç­”æ¡ˆï¼šä¼˜å…ˆä½¿ç”¨ standard_answer
        ground_truth = str(r.get("standard_answer") or "")

        # æ„å»ºæ•°æ®ç‚¹ - ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æ˜¯æ­£ç¡®çš„ç±»å‹
        data_point = {
            "question": str(r["question"]),  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
            "answer": str(r["answer"]),      # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
            "contexts": contexts,            # å­—ç¬¦ä¸²åˆ—è¡¨
            "ground_truth": ground_truth if ground_truth else "æ— æ ‡å‡†ç­”æ¡ˆ"  # å­—ç¬¦ä¸²
        }
        data.append(data_point)

    try:
        # 3/4. ç›´æ¥ç”¨ from_list æ„å»º Datasetï¼Œé¿å… DataFrame ç ´ååˆ—è¡¨ç±»å‹
        dataset = Dataset.from_list(data)

        # 5. é€‰æ‹©è¦è®¡ç®—çš„æŒ‡æ ‡
        metrics_to_compute = []
        if is_rag:
            # RAGç³»ç»Ÿè®¡ç®—æ‰€æœ‰æŒ‡æ ‡
            metrics_to_compute = [
                context_recall,
                context_precision,
                faithfulness,
                answer_relevancy
            ]
        else:
            # é€šç”¨AIåªè®¡ç®—æ— ä¸Šä¸‹æ–‡ä¾èµ–çš„æŒ‡æ ‡
            # æ³¨æ„ï¼šfaithfulness éœ€è¦ contextsï¼Œæ²¡æœ‰ RAG æ—¶æ— æ³•è®¡ç®—
            metrics_to_compute = [
                answer_relevancy
            ]


        print(" ğŸ”„ æ­£åœ¨è®¡ç®—æŒ‡æ ‡ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        #print(metrics_to_compute)

        # 7. åœ¨è°ƒç”¨ evaluate ä¹‹å‰ï¼Œè®¾ç½® RAGAS æ‰€éœ€çš„ OpenAI å…¼å®¹ç¯å¢ƒå˜é‡ï¼ˆç”¨äºå…¶å†…éƒ¨ OpenAIEmbeddingsï¼‰
        #    RAGAS ä¼šä»ç¯å¢ƒå˜é‡è¯»å–å¯†é’¥ä¸ base_url
        os.environ.setdefault("OPENAI_API_KEY", API_KEY)
        os.environ.setdefault("OPENAI_BASE_URL", BASE_URL)
        # å…¼å®¹éƒ¨åˆ†åº“è¯»å– OPENAI_API_BASE
        os.environ.setdefault("OPENAI_API_BASE", BASE_URL)
        # å°†é»˜è®¤åµŒå…¥æ¨¡å‹åˆ‡æ¢ä¸º DashScope çš„ embeddings æ¨¡å‹ï¼Œé¿å…ä½¿ç”¨ text-embedding-ada-002
        os.environ.setdefault("OPENAI_EMBEDDING_MODEL", "text-embedding-v4")

        # 8. æ˜¾å¼ä½¿ç”¨ DashScope çš„ embeddingï¼Œé¿å… RAGAS é»˜è®¤çš„ OpenAI ada-002
        dashscope_embeddings = DashScopeEmbeddings(
            model="text-embedding-v4",
            dashscope_api_key=API_KEY,
        )

        # 9. è°ƒç”¨ evaluate
        metrics = evaluate(
                dataset,
                metrics=metrics_to_compute,
                llm=ragas_llm,  # æ³¨å…¥è‡ªå®šä¹‰LLM
                embeddings=dashscope_embeddings,
                raise_exceptions=False  # é˜²æ­¢å•ä¸ªé—®é¢˜å¤±è´¥å¯¼è‡´æ•´ä¸ªè¯„ä¼°å´©æºƒ
            )

        summary_metrics = {}
        metrics_df = None
        metrics_series_map = {}

        try:
            if hasattr(metrics, "to_pandas"):
                metrics_df = metrics.to_pandas()
                excluded_cols = {"question", "answer", "ground_truth", "contexts"}
                for col in metrics_df.columns:
                    if col in excluded_cols:
                        continue
                    numeric_series = pd.to_numeric(metrics_df[col], errors="coerce")
                    if numeric_series.notna().any():
                        metrics_series_map[col] = numeric_series
                        summary_metrics[col] = float(numeric_series.mean())

            if not summary_metrics and hasattr(metrics, "overall") and isinstance(metrics.overall, dict):
                for name, val in metrics.overall.items():
                    try:
                        summary_metrics[name] = float(val)
                    except Exception:
                        summary_metrics[name] = None
        except Exception:
            summary_metrics = {}

        # å°†æ¯æ¡æ ·æœ¬çš„æŒ‡æ ‡å†™å…¥ç»“æœï¼›è‹¥ç¼ºå¤±åˆ™ä½¿ç”¨æ±‡æ€»
        for idx, r in enumerate(valid_results):
            sample_metrics = {}
            for name, series in metrics_series_map.items():
                value = series.iloc[idx] if idx < len(series) else None
                sample_metrics[name] = float(value) if pd.notna(value) else None
            if not sample_metrics:
                sample_metrics = dict(summary_metrics)
            r["metrics"] = sample_metrics

        print(f"âœ… æŒ‡æ ‡è®¡ç®—æˆåŠŸ! æ ·æœ¬æ•°: {len(valid_results)}")
        return results, summary_metrics

    except Exception as e:
        print(f"âŒ è®¡ç®—RAGASæŒ‡æ ‡å¤±è´¥: {str(e)}")
        traceback.print_exc()
        # è¿”å›å¤±è´¥çš„æŒ‡æ ‡
        empty_metrics = {
            "context_recall": None,
            "context_precision": None,
            "faithfulness": None,
            "answer_relevancy": None
        }
        return results, empty_metrics


# =============== æŠ¥å‘Šç”Ÿæˆå‡½æ•° ===============

def generate_raw_report(rag_results, rag_summary, rag_total_time,
                        general_results, general_summary, general_total_time):
    """ç”ŸæˆåŸå§‹æµ‹è¯•ç»“æœæŠ¥å‘Š"""
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    report_file = os.path.join(REPORT_DIR, f"evaluation_report_{timestamp}.json")

    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    rag_success_count = sum(1 for r in rag_results if r.get("success", True))
    rag_avg_time = rag_total_time / rag_success_count if rag_success_count > 0 else 0
    rag_throughput = rag_success_count / rag_total_time if rag_total_time > 0 else 0

    general_success_count = sum(1 for r in general_results if r.get("success", True))
    general_avg_time = general_total_time / general_success_count if general_success_count > 0 else 0
    general_throughput = general_success_count / general_total_time if general_total_time > 0 else 0

    # æ„å»ºæŠ¥å‘Šç»“æ„
    report = {
        "metadata": {
            "timestamp": timestamp,
            "total_questions": len(rag_results),
            "successful_rag_questions": rag_success_count,
            "successful_general_questions": general_success_count,
            "evaluation_config": {
                "rag_system": "agent_backend.py (with reranking)",
                "general_ai_model": GENERAL_MODEL,
                "ragas_evaluation_model": RAGAS_EVALUATION_MODEL,
                "temperature": GENERAL_TEMPERATURE
            }
        },
        "rag_system": {
            "summary_metrics": {
                **rag_summary,
                "avg_response_time_seconds": rag_avg_time,
                "throughput_requests_per_second": rag_throughput,
                "success_rate": rag_success_count / len(rag_results) if len(rag_results) > 0 else 0
            },
            "detailed_results": rag_results
        },
        "general_ai": {
            "summary_metrics": {
                **general_summary,
                "avg_response_time_seconds": general_avg_time,
                "throughput_requests_per_second": general_throughput,
                "success_rate": general_success_count / len(general_results) if len(general_results) > 0 else 0
            },
            "detailed_results": general_results
        },
        "comparison": {
            "faithfulness_delta": (rag_summary.get("faithfulness", 0) or 0) - (
                        general_summary.get("faithfulness", 0) or 0),
            "answer_relevancy_delta": (rag_summary.get("answer_relevancy", 0) or 0) - (
                        general_summary.get("answer_relevancy", 0) or 0),
            "avg_time_delta": rag_avg_time - general_avg_time
        }
    }

    # ä¿å­˜æŠ¥å‘Š
    try:
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
        print(f"   - æ–‡ä»¶å¤§å°: {os.path.getsize(report_file) / 1024:.2f} KB")
        return report_file
    except Exception as e:
        print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
        traceback.print_exc()
        return None


# =============== ä¸»å‡½æ•° ===============

def main():
    """ä¸»è¯„ä¼°æµç¨‹"""
    print("=" * 60)
    print("ğŸ½ï¸  RAGçƒ¹é¥ªåŠ©æ‰‹ç³»ç»Ÿè¯„ä¼°")
    print("=" * 60)

    # 1. åŠ è½½æµ‹è¯•é—®é¢˜
    print("\nğŸ“¥ æ­¥éª¤1: åŠ è½½æµ‹è¯•é—®é¢˜é›†...")
    try:
        test_questions = load_test_questions(TEST_DATA_PATH)
    except Exception as e:
        print(f"è‡´å‘½é”™è¯¯: æ— æ³•åŠ è½½æµ‹è¯•æ•°æ® - {str(e)}")
        return

    # 2. è¿è¡ŒRAGç³»ç»Ÿæµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ” æ­¥éª¤2: è¯„ä¼°RAGç³»ç»Ÿ (å¸¦æ£€ç´¢å¢å¼º)")
    print("=" * 60)
    rag_results, rag_total_time = run_rag_system(test_questions)

    # 3. è¿è¡Œé€šç”¨AIæµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ” æ­¥éª¤3: è¯„ä¼°é€šç”¨AI (æ— æ£€ç´¢)")
    print("=" * 60)
    general_results, general_total_time = run_general_ai(test_questions)

    # 4. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    print("\n" + "=" * 60)
    print("ğŸ“Š æ­¥éª¤4: è®¡ç®—è¯„ä¼°æŒ‡æ ‡")
    print("=" * 60)
    rag_results, rag_summary = calculate_ragas_metrics(rag_results, is_rag=True)
    general_results, general_summary = calculate_ragas_metrics(general_results, is_rag=False)

    # 5. ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ’¾ æ­¥éª¤5: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)
    report_path = generate_raw_report(
        rag_results, rag_summary, rag_total_time,
        general_results, general_summary, general_total_time
    )

    # 6. æ‰“å°æ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ¯ è¯„ä¼°æ‘˜è¦")
    print("=" * 60)

    if report_path:
        print(f"âœ… è¯„ä¼°å®Œæˆ! æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

        # æ‰“å°å…³é”®æŒ‡æ ‡
        print("\nğŸ“ˆ RAGç³»ç»Ÿå…³é”®æŒ‡æ ‡:")
        print(f"   - ç­”æ¡ˆç›¸å…³æ€§: {rag_summary.get('answer_relevancy', 'N/A'):.4f}" if rag_summary.get(
            'answer_relevancy') is not None else "   - ç­”æ¡ˆç›¸å…³æ€§: N/A")
        print(f"   - äº‹å®å‡†ç¡®æ€§: {rag_summary.get('faithfulness', 'N/A'):.4f}" if rag_summary.get(
            'faithfulness') is not None else "   - äº‹å®å‡†ç¡®æ€§: N/A")
        print(f"   - ä¸Šä¸‹æ–‡å¬å›ç‡: {rag_summary.get('context_recall', 'N/A'):.4f}" if rag_summary.get(
            'context_recall') is not None else "   - ä¸Šä¸‹æ–‡å¬å›ç‡: N/A")
        print(f"   - ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡: {rag_summary.get('context_precision', 'N/A'):.4f}" if rag_summary.get(
            'context_precision') is not None else "   - ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡: N/A")
        print(
            f"   - å¹³å‡å“åº”æ—¶é—´: {rag_total_time / len([r for r in rag_results if r.get('success', True)] or [1]):.2f}ç§’")

        print("\nğŸ“ˆ é€šç”¨AIå…³é”®æŒ‡æ ‡:")
        print(f"   - ç­”æ¡ˆç›¸å…³æ€§: {general_summary.get('answer_relevancy', 'N/A'):.4f}" if general_summary.get(
            'answer_relevancy') is not None else "   - ç­”æ¡ˆç›¸å…³æ€§: N/A")
        print(f"   - äº‹å®å‡†ç¡®æ€§: {general_summary.get('faithfulness', 'N/A'):.4f}" if general_summary.get(
            'faithfulness') is not None else "   - äº‹å®å‡†ç¡®æ€§: N/A")
        print(
            f"   - å¹³å‡å“åº”æ—¶é—´: {general_total_time / len([r for r in general_results if r.get('success', True)] or [1]):.2f}ç§’")
    else:
        print("âŒ è¯„ä¼°æœªå®Œæˆï¼ŒæŠ¥å‘Šç”Ÿæˆå¤±è´¥")

    print("\n" + "=" * 60)
    print("ğŸ‰ è¯„ä¼°æµç¨‹ç»“æŸ")
    print("=" * 60)


# =============== ç¨‹åºå…¥å£ ===============

if __name__ == "__main__":
    main()

