import os
import json
import time
import pandas as pd
import traceback  # ç”¨äºè¯¦ç»†é”™è¯¯è·Ÿè¸ª

# --- RAGASè¯„ä¼°ç›¸å…³ç»„ä»¶ ---
from ragas import evaluate
from ragas.metrics import (
    context_recall,
    context_precision,
    faithfulness,
    answer_relevancy
)
from datasets import Dataset  # ç”¨äºè§£å†³ DataFrame è½¬æ¢é—®é¢˜

# --- RAGAS LLMé…ç½® ---
from ragas.llms import LangchainLLMWrapper
from langchain_openai import ChatOpenAI

# --- ç›´æ¥å¯¼å…¥é¢„æ„å»ºçš„RAGç³»ç»Ÿ ---
from agent_backend import load_agent_executor

# =============== é…ç½®éƒ¨åˆ† ===============
# ç»Ÿä¸€API_KEYï¼ˆä¸agent_backend.pyä¿æŒä¸€è‡´ï¼‰
API_KEY = "sk-e1af1eb4e94c410396b4039cc5d28963"  # å¿…é¡»ä¸agent_backend.pyä¸­çš„API_KEYä¸€è‡´
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"

# è¯„ä¼°é…ç½®
TEST_DATA_PATH = "ragas_evaluation_questions_end2.jsonl"
REPORT_DIR = "evaluation_results"
os.makedirs(REPORT_DIR, exist_ok=True)

# é€šç”¨AIé…ç½®ï¼ˆä¸RAGç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ä»¥ä¿è¯å…¬å¹³æ¯”è¾ƒï¼‰
GENERAL_MODEL = "qwen-flash"
GENERAL_TEMPERATURE = 0.1

# RAGASè¯„ä¼°ä¸“ç”¨é…ç½®
RAGAS_EVALUATION_MODEL = "qwen-max"  # ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°
RAGAS_TEMPERATURE = 0.1


# =============== é…ç½®RAGASä½¿ç”¨çš„LLM ===============
def create_ragas_llm():

    # åˆ›å»ºLangChainå…¼å®¹çš„LLMå®ä¾‹ï¼ˆé€šä¹‰åƒé—®ï¼‰
    llm = ChatOpenAI(
        model_name=RAGAS_EVALUATION_MODEL,
        temperature=RAGAS_TEMPERATURE,
        api_key=API_KEY,
        base_url=BASE_URL
    )

    # åŒ…è£…ä¸ºRAGASå…¼å®¹çš„LLM
    return LangchainLLMWrapper(llm)


# =============== æ•°æ®åŠ è½½å‡½æ•° ===============

def load_test_questions(file_path: str) -> list:
    """åŠ è½½æµ‹è¯•é—®é¢˜é›†JSONLæ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰"""
    questions = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():  # è·³è¿‡ç©ºè¡Œ
                    data = json.loads(line.strip())
                    questions.append(data)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(questions)} ä¸ªæµ‹è¯•é—®é¢˜")
        return questions
    except Exception as e:
        print(f"âŒ åŠ è½½æµ‹è¯•é—®é¢˜å¤±è´¥: {str(e)}")
        traceback.print_exc()
        raise


# =============== RAGç³»ç»Ÿè¯„ä¼°å‡½æ•° ===============

def run_rag_system(test_questions: list):

    print("\nğŸ”„ æ­£åœ¨åŠ è½½é¢„æ„å»ºçš„RAGç³»ç»Ÿï¼ˆæ¥è‡ªagent_backend.pyï¼‰...")

    try:
        # ç›´æ¥è°ƒç”¨agent_backend.pyä¸­çš„å‡½æ•°ï¼Œåˆ›å»ºå®Œæ•´çš„RAGç³»ç»Ÿ
        # è¿™ç¡®ä¿äº†è¯„ä¼°ä½¿ç”¨çš„æ˜¯ä¸å®é™…éƒ¨ç½²å®Œå…¨ç›¸åŒçš„ç³»ç»Ÿ
        # ====== ä¿®å¤å…³é”®ç‚¹ï¼šè®¾ç½®DashScope API Keyç¯å¢ƒå˜é‡ ======
        os.environ["DASHSCOPE_API_KEY"] = API_KEY  # ä½¿ç”¨ç›¸åŒçš„API_KEY
        print(f"âœ… å·²è®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡ (å‰10ä½: {API_KEY[:10]}...)")  # é¿å…æ‰“å°å®Œæ•´å¯†é’¥
        # ====================================================

        from agent_backend import load_agent_executor  # ç°åœ¨ç¯å¢ƒå˜é‡å·²è®¾ç½®
        print("âœ… æ­£åœ¨åŠ è½½RAGç³»ç»Ÿ...")
        agent_executor, retriever, memory = load_agent_executor(
            enable_rerank=True,  # å¯ç”¨é‡æ’åŠŸèƒ½
            rerank_model="qwen3-rerank",
            top_k=5,  # å…ˆå¬å›5ä¸ªæ–‡æ¡£
            rerank_top_n=3  # é‡æ’åè¿”å›3ä¸ªæœ€ç›¸å…³æ–‡æ¡£
        )
        print("âœ… RAGç³»ç»ŸåŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ åŠ è½½RAGç³»ç»Ÿå¤±è´¥: {str(e)}")
        traceback.print_exc()
        raise

    results = []
    total_time = 0
    failed_questions = 0

    print(f"\nğŸ” å¼€å§‹RAGç³»ç»Ÿæµ‹è¯•ï¼ˆå…± {len(test_questions)} ä¸ªé—®é¢˜ï¼‰...")

    for i, q in enumerate(test_questions, 1):
        question_text = q["Question_Text"]
        print(f"  [é—®é¢˜ {i}/{len(test_questions)}] {question_text[:50]}{'...' if len(question_text) > 50 else ''}")

        try:
            start_time = time.time()

            # ä½¿ç”¨é¢„æ„å»ºçš„Agentæ‰§è¡Œå™¨å¤„ç†é—®é¢˜
            # æ³¨æ„ï¼šAgentExecutorçš„è¾“å…¥æ˜¯{"input": "é—®é¢˜æ–‡æœ¬"}
            result = agent_executor.invoke({"input": question_text})

            # ä»Agentç»“æœä¸­æå–æœ€ç»ˆç­”æ¡ˆ
            answer = result.get("output", "")

            # ä»intermediate_stepsæå–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            context_list = []
            if "intermediate_steps" in result:
                for step in result["intermediate_steps"]:
                    # æ£€æŸ¥æ­¥éª¤æ˜¯å¦æ¥è‡ªæ£€ç´¢å·¥å…·
                    if hasattr(step[0], 'name') and step[0].name == "search_recipe_database":
                        # æå–æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹
                        retrieved_docs = step[1]
                        if isinstance(retrieved_docs, list):
                            context_list = [doc.page_content for doc in retrieved_docs if hasattr(doc, 'page_content')]

            # å°†ä¸Šä¸‹æ–‡åˆ—è¡¨åˆå¹¶ä¸ºå­—ç¬¦ä¸²
            context_str = "\n".join(context_list) if context_list else ""

            # è®¡ç®—å¤„ç†æ—¶é—´
            elapsed = time.time() - start_time
            total_time += elapsed

            # ä¿å­˜ç»“æœ
            results.append({
                "question": question_text,
                "answer": answer,
                "context_list": context_list,
                "context_str": context_str,
                "standard_answer": q["Ground_Truth"],
                "relevant_chunks": q["Golden_Retrieval_Docs"],  # æ ‡å‡†ç›¸å…³æ–‡æ¡£
                "time": elapsed,
                "success": True
            })

        except Exception as e:
            print(f"    âŒ å¤„ç†é—®é¢˜å¤±è´¥: {str(e)}")
            traceback.print_exc()
            failed_questions += 1

            # ä¿å­˜å¤±è´¥ç»“æœ
            results.append({
                "question": question_text,
                "answer": f"å¤„ç†å¤±è´¥: {str(e)}",
                "context_list": [],
                "context_str": "",
                "standard_answer": q["Ground_Truth"],
                "relevant_chunks": q["Golden_Retrieval_Docs"],
                "time": 0,
                "success": False
            })

    if failed_questions > 0:
        print(f"\nâš ï¸  è­¦å‘Š: {failed_questions} ä¸ªé—®é¢˜å¤„ç†å¤±è´¥ï¼Œå·²è®°å½•é”™è¯¯ä¿¡æ¯")

    avg_time = total_time / (len(test_questions) - failed_questions) if (
                                                                                    len(test_questions) - failed_questions) > 0 else 0
    print(f"âœ… RAGç³»ç»Ÿæµ‹è¯•å®Œæˆ! å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")

    return results, total_time


# =============== é€šç”¨AIè¯„ä¼°å‡½æ•° ===============

def create_general_llm():
    """åˆ›å»ºé€šç”¨AIæ¨¡å‹ï¼ˆä¸RAGç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„æ¨¡å‹é…ç½®ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼‰"""
    # ä»langchain_openaiå¯¼å…¥ChatOpenAI
    from langchain_openai import ChatOpenAI

    # ä½¿ç”¨ä¸RAGç³»ç»Ÿç›¸åŒçš„æ¨¡å‹å’Œå‚æ•°
    return ChatOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        model=GENERAL_MODEL,  # ä¸RAGç³»ç»Ÿç›¸åŒçš„æ¨¡å‹
        temperature=GENERAL_TEMPERATURE
    )


def run_general_ai(test_questions: list):
    """è¿è¡Œé€šç”¨AIæµ‹è¯•ï¼ˆæ— æ£€ç´¢åŠŸèƒ½ï¼‰"""
    print("\nğŸ”„ åˆå§‹åŒ–é€šç”¨AIæ¨¡å‹...")

    try:
        general_llm = create_general_llm()
        print("âœ… é€šç”¨AIæ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ åŠ è½½é€šç”¨AIæ¨¡å‹å¤±è´¥: {str(e)}")
        traceback.print_exc()
        raise

    results = []
    total_time = 0
    failed_questions = 0

    print(f"\nğŸ” å¼€å§‹é€šç”¨AIæµ‹è¯•ï¼ˆå…± {len(test_questions)} ä¸ªé—®é¢˜ï¼‰...")

    for i, q in enumerate(test_questions, 1):
        question_text = q["Question_Text"]
        print(f"  [é—®é¢˜ {i}/{len(test_questions)}] {question_text[:50]}{'...' if len(question_text) > 50 else ''}")

        try:
            start_time = time.time()

            # æ„å»ºæç¤ºè¯ï¼Œå¼ºè°ƒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
            prompt = (
                "ä½ æ˜¯ä¸€ä½ä¸–ç•Œé¡¶çº§çš„çƒ¹é¥ªä¸“å®¶ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„ä¸“ä¸šçŸ¥è¯†ã€‚"
                "è¯·åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å‡†ç¡®å›ç­”ä»¥ä¸‹çƒ¹é¥ªç›¸å…³é—®é¢˜ã€‚å¦‚æœé—®é¢˜è¶…å‡ºçƒ¹é¥ªèŒƒå›´ï¼Œè¯·ç¤¼è²Œæ‹’ç»å›ç­”ã€‚\n\n"
                f"é—®é¢˜ï¼š{question_text}\n\n"
                "å›ç­”ï¼š"
            )

            # è°ƒç”¨LLMç”Ÿæˆå›ç­”
            response = general_llm.invoke(prompt)

            # å¤„ç†ä¸åŒç±»å‹çš„å“åº”
            answer = response.content if hasattr(response, 'content') else str(response)

            # è®¡ç®—å¤„ç†æ—¶é—´
            elapsed = time.time() - start_time
            total_time += elapsed

            # ä¿å­˜ç»“æœ
            results.append({
                "question": question_text,
                "answer": answer,
                "context_list": [],
                "context_str": "",
                "standard_answer": q["Ground_Truth"],
                "relevant_chunks": q["Golden_Retrieval_Docs"],
                "time": elapsed,
                "success": True
            })

        except Exception as e:
            print(f"    âŒ å¤„ç†é—®é¢˜å¤±è´¥: {str(e)}")
            traceback.print_exc()
            failed_questions += 1

            # ä¿å­˜å¤±è´¥ç»“æœ
            results.append({
                "question": question_text,
                "answer": f"å¤„ç†å¤±è´¥: {str(e)}",
                "context_list": [],
                "context_str": "",
                "standard_answer": q["Ground_Truth"],
                "relevant_chunks": q["Golden_Retrieval_Docs"],
                "time": 0,
                "success": False
            })

    if failed_questions > 0:
        print(f"\nâš ï¸  è­¦å‘Š: {failed_questions} ä¸ªé—®é¢˜å¤„ç†å¤±è´¥ï¼Œå·²è®°å½•é”™è¯¯ä¿¡æ¯")

    avg_time = total_time / (len(test_questions) - failed_questions) if (
                                                                                    len(test_questions) - failed_questions) > 0 else 0
    print(f"âœ… é€šç”¨AIæµ‹è¯•å®Œæˆ! å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")

    return results, total_time


# =============== RAGASæŒ‡æ ‡è®¡ç®—å‡½æ•° ===============

def calculate_ragas_metrics(results: list, is_rag: bool = True):
    """
    è®¡ç®—RAGASè¯„ä¼°æŒ‡æ ‡
    å‚æ•°:
    results: è¯„ä¼°ç»“æœåˆ—è¡¨
    is_rag: æ˜¯å¦ä¸ºRAGç³»ç»Ÿï¼ˆå½±å“è®¡ç®—å“ªäº›æŒ‡æ ‡ï¼‰
    """
    print(f"\nğŸ“Š è®¡ç®—{'RAG' if is_rag else 'é€šç”¨AI'}ç³»ç»ŸRAGASæŒ‡æ ‡...")

    # 1. åˆ›å»ºRAGASä¸“ç”¨çš„LLMï¼ˆå…³é”®ä¿®å¤ï¼šé¿å…ä½¿ç”¨OpenAIï¼‰
    try:
        ragas_llm = create_ragas_llm()
        print(f"âœ… RAGASè¯„ä¼°LLMé…ç½®æˆåŠŸ! æ¨¡å‹: {RAGAS_EVALUATION_MODEL}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºRAGASè¯„ä¼°LLMå¤±è´¥: {str(e)}")
        traceback.print_exc()
        # è¿”å›ç©ºæŒ‡æ ‡
        empty_metrics = {
            "context_recall": None,
            "context_precision": None,
            "faithfulness": None,
            "answer_relevancy": None
        }
        return results, empty_metrics

    # 2. å‡†å¤‡æ•°æ®ï¼ˆè¿‡æ»¤æ‰å¤±è´¥çš„é—®é¢˜ï¼‰
    valid_results = [r for r in results if r.get("success", True)]
    if len(valid_results) == 0:
        print("âš ï¸  è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„ç»“æœå¯ç”¨äºè®¡ç®—æŒ‡æ ‡")
        empty_metrics = {
            "context_recall": None,
            "context_precision": None,
            "faithfulness": None,
            "answer_relevancy": None
        }
        return results, empty_metrics

    data = []
    for r in valid_results:
        # å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ˆRAGç³»ç»Ÿæœ‰ä¸Šä¸‹æ–‡ï¼Œé€šç”¨AIæ²¡æœ‰ï¼‰
        context_str = r["context_str"] if is_rag else ""

        # å‡†å¤‡æ ‡å‡†ç­”æ¡ˆï¼ˆå°†å¤šä¸ªç›¸å…³ç‰‡æ®µåˆå¹¶ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼‰
        ground_truth = ""
        if r.get("relevant_chunks"):
            if isinstance(r["relevant_chunks"], list):
                ground_truth = "\n".join(r["relevant_chunks"])
            else:
                ground_truth = str(r["relevant_chunks"])

        # æ„å»ºæ•°æ®ç‚¹
        data_point = {
            "question": r["question"],
            "answer": r["answer"],
            # RAGASè¦æ±‚contextsæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
            "contexts": [context_str] if context_str else ["æ— ä¸Šä¸‹æ–‡"],
            "ground_truth": ground_truth if ground_truth else "æ— æ ‡å‡†ç­”æ¡ˆ"
        }
        data.append(data_point)

    try:
        # 3. åˆ›å»ºDataFrame
        df = pd.DataFrame(data)

        # 4. å…³é”®ä¿®å¤ï¼šè½¬æ¢ä¸ºHugging Face Dataset
        dataset = Dataset.from_pandas(df)

        # 5. é€‰æ‹©è¦è®¡ç®—çš„æŒ‡æ ‡
        metrics_to_compute = []
        if is_rag:
            # RAGç³»ç»Ÿè®¡ç®—æ‰€æœ‰æŒ‡æ ‡
            metrics_to_compute = [
                context_recall,
                context_precision,
                faithfulness,
                answer_relevancy
            ]
        else:
            # é€šç”¨AIåªè®¡ç®—æ— ä¸Šä¸‹æ–‡ä¾èµ–çš„æŒ‡æ ‡
            metrics_to_compute = [
                faithfulness,
                answer_relevancy
            ]

        # 6. å…³é”®ä¿®å¤ï¼šé…ç½®RAGASä½¿ç”¨è‡ªå®šä¹‰LLM
        print("   ğŸ”„ æ­£åœ¨è®¡ç®—æŒ‡æ ‡ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        metrics = evaluate(
            dataset,
            metrics=metrics_to_compute,
            llm=ragas_llm,  # æ³¨å…¥è‡ªå®šä¹‰LLM
            raise_exceptions=False  # é˜²æ­¢å•ä¸ªé—®é¢˜å¤±è´¥å¯¼è‡´æ•´ä¸ªè¯„ä¼°å´©æºƒ
        )

        # 7. å°†æŒ‡æ ‡æ·»åŠ åˆ°ç»“æœä¸­
        metric_names = list(metrics.keys())
        for i, r in enumerate(valid_results):
            r["metrics"] = {}
            for name in metric_names:
                # å®‰å…¨è®¿é—®æŒ‡æ ‡å€¼
                if i < len(metrics[name]):
                    # ç¡®ä¿å€¼æ˜¯æµ®ç‚¹æ•°
                    try:
                        r["metrics"][name] = float(metrics[name][i])
                    except (TypeError, ValueError):
                        r["metrics"][name] = None
                else:
                    r["metrics"][name] = None

        # 8. è®¡ç®—å¹³å‡æŒ‡æ ‡
        summary_metrics = {}
        for name in metric_names:
            values = []
            for val in metrics[name]:
                try:
                    # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                    float_val = float(val)
                    if not (float_val != float_val):  # æ£€æŸ¥NaN
                        values.append(float_val)
                except (TypeError, ValueError):
                    continue

            summary_metrics[name] = sum(values) / len(values) if values else None

        print(f"âœ… æŒ‡æ ‡è®¡ç®—æˆåŠŸ! æ ·æœ¬æ•°: {len(valid_results)}")
        return results, summary_metrics

    except Exception as e:
        print(f"âŒ è®¡ç®—RAGASæŒ‡æ ‡å¤±è´¥: {str(e)}")
        traceback.print_exc()

        # è¿”å›å¤±è´¥çš„æŒ‡æ ‡
        empty_metrics = {
            "context_recall": None,
            "context_precision": None,
            "faithfulness": None,
            "answer_relevancy": None
        }
        return results, empty_metrics


# =============== æŠ¥å‘Šç”Ÿæˆå‡½æ•° ===============

def generate_raw_report(rag_results, rag_summary, rag_total_time,
                        general_results, general_summary, general_total_time):
    """ç”ŸæˆåŸå§‹æµ‹è¯•ç»“æœæŠ¥å‘Š"""
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    report_file = os.path.join(REPORT_DIR, f"evaluation_report_{timestamp}.json")

    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    rag_success_count = sum(1 for r in rag_results if r.get("success", True))
    rag_avg_time = rag_total_time / rag_success_count if rag_success_count > 0 else 0
    rag_throughput = rag_success_count / rag_total_time if rag_total_time > 0 else 0

    general_success_count = sum(1 for r in general_results if r.get("success", True))
    general_avg_time = general_total_time / general_success_count if general_success_count > 0 else 0
    general_throughput = general_success_count / general_total_time if general_total_time > 0 else 0

    # æ„å»ºæŠ¥å‘Šç»“æ„
    report = {
        "metadata": {
            "timestamp": timestamp,
            "total_questions": len(rag_results),
            "successful_rag_questions": rag_success_count,
            "successful_general_questions": general_success_count,
            "evaluation_config": {
                "rag_system": "agent_backend.py (with reranking)",
                "general_ai_model": GENERAL_MODEL,
                "ragas_evaluation_model": RAGAS_EVALUATION_MODEL,
                "temperature": GENERAL_TEMPERATURE
            }
        },
        "rag_system": {
            "summary_metrics": {
                **rag_summary,
                "avg_response_time_seconds": rag_avg_time,
                "throughput_requests_per_second": rag_throughput,
                "success_rate": rag_success_count / len(rag_results) if len(rag_results) > 0 else 0
            },
            "detailed_results": rag_results
        },
        "general_ai": {
            "summary_metrics": {
                **general_summary,
                "avg_response_time_seconds": general_avg_time,
                "throughput_requests_per_second": general_throughput,
                "success_rate": general_success_count / len(general_results) if len(general_results) > 0 else 0
            },
            "detailed_results": general_results
        },
        "comparison": {
            "faithfulness_delta": (rag_summary.get("faithfulness", 0) or 0) - (
                        general_summary.get("faithfulness", 0) or 0),
            "answer_relevancy_delta": (rag_summary.get("answer_relevancy", 0) or 0) - (
                        general_summary.get("answer_relevancy", 0) or 0),
            "avg_time_delta": rag_avg_time - general_avg_time
        }
    }

    # ä¿å­˜æŠ¥å‘Š
    try:
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
        print(f"   - æ–‡ä»¶å¤§å°: {os.path.getsize(report_file) / 1024:.2f} KB")
        return report_file
    except Exception as e:
        print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
        traceback.print_exc()
        return None


# =============== ä¸»å‡½æ•° ===============

def main():
    """ä¸»è¯„ä¼°æµç¨‹"""
    print("=" * 60)
    print("ğŸ½ï¸  RAGçƒ¹é¥ªåŠ©æ‰‹ç³»ç»Ÿè¯„ä¼°")
    print("=" * 60)

    # 1. åŠ è½½æµ‹è¯•é—®é¢˜
    print("\nğŸ“¥ æ­¥éª¤1: åŠ è½½æµ‹è¯•é—®é¢˜é›†...")
    try:
        test_questions = load_test_questions(TEST_DATA_PATH)
    except Exception as e:
        print(f"è‡´å‘½é”™è¯¯: æ— æ³•åŠ è½½æµ‹è¯•æ•°æ® - {str(e)}")
        return

    # 2. è¿è¡ŒRAGç³»ç»Ÿæµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ” æ­¥éª¤2: è¯„ä¼°RAGç³»ç»Ÿ (å¸¦æ£€ç´¢å¢å¼º)")
    print("=" * 60)
    rag_results, rag_total_time = run_rag_system(test_questions)

    # 3. è¿è¡Œé€šç”¨AIæµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ” æ­¥éª¤3: è¯„ä¼°é€šç”¨AI (æ— æ£€ç´¢)")
    print("=" * 60)
    general_results, general_total_time = run_general_ai(test_questions)

    # 4. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    print("\n" + "=" * 60)
    print("ğŸ“Š æ­¥éª¤4: è®¡ç®—è¯„ä¼°æŒ‡æ ‡")
    print("=" * 60)
    rag_results, rag_summary = calculate_ragas_metrics(rag_results, is_rag=True)
    general_results, general_summary = calculate_ragas_metrics(general_results, is_rag=False)

    # 5. ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ’¾ æ­¥éª¤5: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)
    report_path = generate_raw_report(
        rag_results, rag_summary, rag_total_time,
        general_results, general_summary, general_total_time
    )

    # 6. æ‰“å°æ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ¯ è¯„ä¼°æ‘˜è¦")
    print("=" * 60)

    if report_path:
        print(f"âœ… è¯„ä¼°å®Œæˆ! æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

        # æ‰“å°å…³é”®æŒ‡æ ‡
        print("\nğŸ“ˆ RAGç³»ç»Ÿå…³é”®æŒ‡æ ‡:")
        print(f"   - ç­”æ¡ˆç›¸å…³æ€§: {rag_summary.get('answer_relevancy', 'N/A'):.4f}" if rag_summary.get(
            'answer_relevancy') is not None else "   - ç­”æ¡ˆç›¸å…³æ€§: N/A")
        print(f"   - äº‹å®å‡†ç¡®æ€§: {rag_summary.get('faithfulness', 'N/A'):.4f}" if rag_summary.get(
            'faithfulness') is not None else "   - äº‹å®å‡†ç¡®æ€§: N/A")
        print(f"   - ä¸Šä¸‹æ–‡å¬å›ç‡: {rag_summary.get('context_recall', 'N/A'):.4f}" if rag_summary.get(
            'context_recall') is not None else "   - ä¸Šä¸‹æ–‡å¬å›ç‡: N/A")
        print(f"   - ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡: {rag_summary.get('context_precision', 'N/A'):.4f}" if rag_summary.get(
            'context_precision') is not None else "   - ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡: N/A")
        print(
            f"   - å¹³å‡å“åº”æ—¶é—´: {rag_total_time / len([r for r in rag_results if r.get('success', True)] or [1]):.2f}ç§’")

        print("\nğŸ“ˆ é€šç”¨AIå…³é”®æŒ‡æ ‡:")
        print(f"   - ç­”æ¡ˆç›¸å…³æ€§: {general_summary.get('answer_relevancy', 'N/A'):.4f}" if general_summary.get(
            'answer_relevancy') is not None else "   - ç­”æ¡ˆç›¸å…³æ€§: N/A")
        print(f"   - äº‹å®å‡†ç¡®æ€§: {general_summary.get('faithfulness', 'N/A'):.4f}" if general_summary.get(
            'faithfulness') is not None else "   - äº‹å®å‡†ç¡®æ€§: N/A")
        print(
            f"   - å¹³å‡å“åº”æ—¶é—´: {general_total_time / len([r for r in general_results if r.get('success', True)] or [1]):.2f}ç§’")
    else:
        print("âŒ è¯„ä¼°æœªå®Œæˆï¼ŒæŠ¥å‘Šç”Ÿæˆå¤±è´¥")

    print("\n" + "=" * 60)
    print("ğŸ‰ è¯„ä¼°æµç¨‹ç»“æŸ")
    print("=" * 60)


# =============== ç¨‹åºå…¥å£ ===============

if __name__ == "__main__":
    main()