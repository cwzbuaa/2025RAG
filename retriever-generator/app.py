import streamlit as st
import time
from agent_backend import load_agent_executor
from langchain.memory import ConversationBufferMemory
#   streamlit run retriever-generator/app.py
# --- 1. é¡µé¢è®¾ç½® ---
st.set_page_config(
    page_title="èˆªå°å¨ - ä½ çš„AIçƒ¹é¥ªåŠ©æ‰‹",
    page_icon="ğŸ³",
    layout="wide"
)
st.title("èˆªå°å¨ ğŸ³ - ä½ çš„AIçƒ¹é¥ªåŠ©æ‰‹")

# --- 2. ä¾§è¾¹æ ï¼šé…ç½® ---
with st.sidebar:
    st.header("âš™ï¸ é…ç½®")
    
    # æ˜¯å¦å¯ç”¨é‡æ’
    enable_rerank = st.checkbox(
        "å¯ç”¨é‡æ’",
        value=st.session_state.get("enable_rerank", True),
        help="æ˜¯å¦ä½¿ç”¨é‡æ’æ¨¡å‹å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’"
    )
    st.session_state.enable_rerank = enable_rerank
    
    if enable_rerank:
        # é‡æ’æ¨¡å‹é€‰æ‹©
        rerank_model = st.selectbox(
            "é‡æ’æ¨¡å‹",
            options=["qwen3-rerank", "qwen-rerank"],
            index=0,
            help="é€‰æ‹©ç”¨äºé‡æ’çš„æ¨¡å‹"
        )
        st.session_state.rerank_model = rerank_model
        
        # Top K å‚æ•°ï¼ˆé‡æ’æ—¶ä½¿ç”¨ï¼‰
        top_k = st.number_input(
            "Top K (å¬å›æ–‡æ¡£æ•°)",
            min_value=1,
            max_value=20,
            value=st.session_state.get("top_k", 5),
            step=1,
            help="å…ˆä»å‘é‡åº“å¬å›çš„æ–‡æ¡£æ•°é‡"
        )
        st.session_state.top_k = top_k
        
        # Rerank Top N å‚æ•°
        rerank_top_n = st.number_input(
            "Rerank Top N (é‡æ’åè¿”å›æ•°)",
            min_value=1,
            max_value=10,
            value=st.session_state.get("rerank_top_n", 3),
            step=1,
            help="é‡æ’åè¿”å›çš„æ–‡æ¡£æ•°é‡"
        )
        st.session_state.rerank_top_n = rerank_top_n
    else:
        # ä¸ä½¿ç”¨é‡æ’æ—¶ï¼Œç›´æ¥è®¾ç½®è¿”å›æ–‡æ¡£æ•°
        top_k = st.number_input(
            "Top K (è¿”å›æ–‡æ¡£æ•°)",
            min_value=1,
            max_value=20,
            value=st.session_state.get("top_k", 3),
            step=1,
            help="ç›´æ¥ä»å‘é‡åº“è¿”å›çš„æ–‡æ¡£æ•°é‡"
        )
        st.session_state.top_k = top_k
        # ä¸ä½¿ç”¨é‡æ’æ—¶ï¼Œè®¾ç½®é»˜è®¤å€¼
        st.session_state.rerank_model = "qwen3-rerank"
        st.session_state.rerank_top_n = top_k
    
    st.divider()
    
    # æ–°å¯¹è¯æŒ‰é’®
    if st.button("ğŸ”„ å¼€å§‹æ–°å¯¹è¯", use_container_width=True):
        st.session_state.messages = [
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯èˆªå°å¨ï¼Œä¸€ä¸ªAIçƒ¹é¥ªä¸“å®¶ã€‚ä½ æƒ³åšä»€ä¹ˆèœï¼Ÿæˆ–è€…å‘Šè¯‰æˆ‘ä½ æœ‰ä»€ä¹ˆé£Ÿæï¼Ÿ"}
        ]
        st.session_state.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        # æ¸…é™¤ agent_executorï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½ï¼ˆä½¿ç”¨æ–°çš„ memoryï¼‰
        if "agent_executor" in st.session_state:
            del st.session_state.agent_executor
        if "rerank_retriever" in st.session_state:
            del st.session_state.rerank_retriever
        st.session_state.performance_metrics = []  # é‡ç½®æ€§èƒ½æŒ‡æ ‡
        st.rerun()
    
    st.divider()
    
    # æ€§èƒ½æŒ‡æ ‡æ˜¾ç¤º
    st.header("ğŸ“Š æ€§èƒ½æŒ‡æ ‡")
    if "performance_metrics" in st.session_state and st.session_state.performance_metrics:
        metrics = st.session_state.performance_metrics
        avg_retrieval_time = sum(m["retrieval_time"] for m in metrics) / len(metrics)
        avg_generation_time = sum(m["generation_time"] for m in metrics) / len(metrics)
        avg_total_time = sum(m["total_time"] for m in metrics) / len(metrics)
        throughput = len(metrics) / sum(m["total_time"] for m in metrics) if sum(m["total_time"] for m in metrics) > 0 else 0
        
        st.metric("å¹³å‡æ£€ç´¢æ—¶é—´", f"{avg_retrieval_time:.3f}ç§’")
        st.metric("å¹³å‡ç”Ÿæˆæ—¶é—´", f"{avg_generation_time:.3f}ç§’")
        st.metric("å¹³å‡æ€»å»¶è¿Ÿ", f"{avg_total_time:.3f}ç§’")
        st.metric("ååé‡", f"{throughput:.2f} è¯·æ±‚/ç§’")
    else:
        st.info("æš‚æ— æ€§èƒ½æ•°æ®")

# --- 3. åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ ---
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯èˆªå°å¨ï¼Œä¸€ä¸ªAIçƒ¹é¥ªä¸“å®¶ã€‚ä½ æƒ³åšä»€ä¹ˆèœï¼Ÿæˆ–è€…å‘Šè¯‰æˆ‘ä½ æœ‰ä»€ä¹ˆé£Ÿæï¼Ÿ"}
    ]

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )

if "performance_metrics" not in st.session_state:
    st.session_state.performance_metrics = []

# åˆå§‹åŒ–é…ç½®å‚æ•°
if "enable_rerank" not in st.session_state:
    st.session_state.enable_rerank = True
if "rerank_model" not in st.session_state:
    st.session_state.rerank_model = "qwen3-rerank"
if "top_k" not in st.session_state:
    st.session_state.top_k = 5
if "rerank_top_n" not in st.session_state:
    st.session_state.rerank_top_n = 3

# --- 4. åŠ è½½ Agent ---
try:
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½ Agentï¼ˆå‚æ•°æ”¹å˜æ—¶æˆ– agent_executor ä¸å­˜åœ¨/ä¸º Noneï¼‰
    config_changed = (
        st.session_state.get("current_enable_rerank") != st.session_state.enable_rerank
        or st.session_state.get("current_rerank_model") != st.session_state.rerank_model
        or st.session_state.get("current_top_k") != st.session_state.top_k
        or st.session_state.get("current_rerank_top_n") != st.session_state.rerank_top_n
    )
    
    if ("agent_executor" not in st.session_state 
        or st.session_state.get("agent_executor") is None
        or config_changed):
        with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œ Agent..."):
            agent_executor, retriever, memory = load_agent_executor(
                memory=st.session_state.memory,
                enable_rerank=st.session_state.enable_rerank,
                rerank_model=st.session_state.rerank_model,
                top_k=st.session_state.top_k,
                rerank_top_n=st.session_state.rerank_top_n
            )
            st.session_state.agent_executor = agent_executor
            st.session_state.rerank_retriever = retriever  # å¯èƒ½æ˜¯ rerank_retriever æˆ–æ™®é€š retriever
            st.session_state.memory = memory
            st.session_state.current_enable_rerank = st.session_state.enable_rerank
            st.session_state.current_rerank_model = st.session_state.rerank_model
            st.session_state.current_top_k = st.session_state.top_k
            st.session_state.current_rerank_top_n = st.session_state.rerank_top_n
            st.success("âœ… Agent åŠ è½½æˆåŠŸï¼")
except Exception as e:
    st.error(f"åŠ è½½ Agent æ—¶å‡ºé”™: {e}")
    st.stop()

# ç¡®ä¿ agent_executor å·²åŠ è½½
if st.session_state.get("agent_executor") is None:
    st.error("âŒ Agent æœªæ­£ç¡®åŠ è½½ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•")
    st.stop()

# --- 5. æ˜¾ç¤ºèŠå¤©å†å² ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- 6. å¤„ç†ç”¨æˆ·è¾“å…¥ ---
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    
    # 6.1 å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å†å²å¹¶æ˜¾ç¤º
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 6.2 è·å– Agent çš„å›ç­”ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰
    with st.chat_message("assistant"):
        with st.spinner("èˆªå°å¨æ­£åœ¨æ€è€ƒä¸­..."):
            try:
                # é‡ç½®æ£€ç´¢æ—¶é—´è®°å½•
                if hasattr(st.session_state.rerank_retriever, 'reset_retrieval_times'):
                    st.session_state.rerank_retriever.reset_retrieval_times()
                
                # è®°å½•å¼€å§‹æ—¶é—´
                total_start_time = time.time()
                
                # å‡†å¤‡è¾“å…¥ï¼ˆåŒ…å«å†å²æ¶ˆæ¯ï¼‰
                input_dict = {"input": prompt}
                if st.session_state.memory:
                    # è·å–å†å²æ¶ˆæ¯ï¼ˆå³ä½¿ä¸ºç©ºä¹Ÿè¦ä¼ é€’ï¼Œå› ä¸º prompt éœ€è¦ chat_history å˜é‡ï¼‰
                    chat_history = st.session_state.memory.chat_memory.messages
                    input_dict["chat_history"] = chat_history if chat_history else []
                
                # æ‰§è¡ŒAgentï¼ˆåŒ…å«æ£€ç´¢å’Œç”Ÿæˆï¼‰
                response = st.session_state.agent_executor.invoke(input_dict)
                total_end_time = time.time()
                total_time = total_end_time - total_start_time
                
                # è®¡ç®—æ£€ç´¢æ—¶é—´ï¼ˆä»æ£€ç´¢å™¨ä¸­è·å–ï¼‰
                retrieval_time = 0
                if hasattr(st.session_state.rerank_retriever, 'get_total_retrieval_time'):
                    retrieval_time = st.session_state.rerank_retriever.get_total_retrieval_time()
                else:
                    # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œåˆ™é€šè¿‡ä¸­é—´æ­¥éª¤æ•°é‡ä¼°ç®—
                    if "intermediate_steps" in response and response["intermediate_steps"]:
                        num_retrieval_steps = len([s for s in response["intermediate_steps"] 
                                                   if isinstance(s, tuple) and hasattr(s[0], 'tool')])
                        # ä¼°ç®—ï¼šæ¯æ¬¡æ£€ç´¢ï¼ˆåŒ…æ‹¬å‘é‡æ£€ç´¢å’Œé‡æ’ï¼‰å¤§çº¦éœ€è¦0.3-0.5ç§’
                        retrieval_time = num_retrieval_steps * 0.4
                        # å¦‚æœæ€»æ—¶é—´å¾ˆçŸ­ï¼Œåˆ™æŒ‰æ¯”ä¾‹åˆ†é…
                        if retrieval_time > total_time * 0.8:
                            retrieval_time = total_time * 0.6  # æ£€ç´¢å 60%
                
                # è®¡ç®—ç”Ÿæˆæ—¶é—´ï¼ˆæ€»æ—¶é—´å‡å»æ£€ç´¢æ—¶é—´ï¼‰
                generation_time = max(0, total_time - retrieval_time)
                
                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                performance_metric = {
                    "retrieval_time": retrieval_time,
                    "generation_time": generation_time,
                    "total_time": total_time
                }
                st.session_state.performance_metrics.append(performance_metric)
                
                final_answer = response["output"]
                
                # æ›´æ–°è®°å¿†
                if st.session_state.memory:
                    st.session_state.memory.chat_memory.add_user_message(prompt)
                    st.session_state.memory.chat_memory.add_ai_message(final_answer)
                
                # æ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆ
                st.markdown(final_answer)
                
                # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
                with st.expander("ğŸ“Š æœ¬æ¬¡æŸ¥è¯¢æ€§èƒ½æŒ‡æ ‡"):
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("æ£€ç´¢æ—¶é—´", f"{retrieval_time:.3f}ç§’")
                    with col2:
                        st.metric("ç”Ÿæˆæ—¶é—´", f"{generation_time:.3f}ç§’")
                    with col3:
                        st.metric("æ€»å»¶è¿Ÿ", f"{total_time:.3f}ç§’")

                # [å¯é€‰] æ˜¾ç¤º Agent çš„æ€è€ƒè¿‡ç¨‹å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
                with st.expander("ğŸ” ç‚¹å‡»æŸ¥çœ‹èˆªå°å¨çš„æ€è€ƒè¿‡ç¨‹ (æ£€ç´¢åˆ°çš„èµ„æ–™)"):
                    if "intermediate_steps" in response and response["intermediate_steps"]:
                        for i, step in enumerate(response["intermediate_steps"]):
                            # Streamlit åœ¨å¤„ç† LangChain çš„ AgentAction å’Œ Observation æ—¶éœ€è¦ä¸€ç‚¹å¸®åŠ©
                            if isinstance(step, tuple):
                                action, observation = step
                                
                                if hasattr(action, 'tool'):
                                    st.markdown(f"**æ­¥éª¤ {i+1}: æ£€ç´¢**")
                                    st.code(f"å·¥å…·: {action.tool}\nè¾“å…¥: {action.tool_input}", language="text")
                                    st.markdown(f"**æ£€ç´¢ç»“æœ:**")
                                    if isinstance(observation, list):
                                        for j, doc in enumerate(observation):
                                            score = doc.metadata.get('score', 0) if hasattr(doc, 'metadata') else 0
                                            st.info(f"**æ–‡æ¡£ {j+1}** (ç›¸å…³æ€§åˆ†æ•°: {score:.4f})\n\n{doc.page_content[:300]}...")
                                    else:
                                        st.write(observation)
                                else:
                                    st.write(f"æ­¥éª¤ {i+1} (éå·¥å…·è°ƒç”¨): {step}")
                            else:
                                st.write(f"æ­¥éª¤ {i+1}: {step}")
                    else:
                        st.write("Agent æœªä½¿ç”¨æ£€ç´¢å·¥å…·æˆ–æœªè¿”å›ä¸­é—´æ­¥éª¤ã€‚")

                # 6.3 å°† Agent çš„å›ç­”æ·»åŠ åˆ°å†å²
                st.session_state.messages.append({"role": "assistant", "content": final_answer})

            except Exception as e:
                st.error(f"Agent æ‰§è¡Œå‡ºé”™: {e}")
                import traceback
                st.code(traceback.format_exc())
